{"paper_id": "e78b9e8104eddf89c4e28b6e746d8505b2a6e8bc", "metadata": {"title": "An encoder-decoder-based method for COVID-19 lung infection segmentation", "authors": [{"first": "Omar", "middle": [], "last": "Elharrouss", "suffix": "", "affiliation": {"laboratory": "", "institution": "Qatar University", "location": {"settlement": "Doha, Doha", "region": "Qatar", "country": "Qatar"}}, "email": ""}, {"first": "Noor", "middle": [], "last": "Almaadeed", "suffix": "", "affiliation": {"laboratory": "", "institution": "Qatar University", "location": {"settlement": "Doha, Doha", "region": "Qatar", "country": "Qatar"}}, "email": ""}, {"first": "Nandhini", "middle": [], "last": "Subramanian", "suffix": "", "affiliation": {"laboratory": "", "institution": "Qatar University", "location": {"settlement": "Doha, Doha", "region": "Qatar", "country": "Qatar"}}, "email": ""}, {"first": "Somaya", "middle": [], "last": "Al-Maadeed", "suffix": "", "affiliation": {"laboratory": "", "institution": "Qatar University", "location": {"settlement": "Doha, Doha", "region": "Qatar", "country": "Qatar"}}, "email": ""}]}, "abstract": [{"text": "The novelty of the COVID-19 disease and the speed of spread has created a colossal chaos, impulse among researchers worldwide to exploit all the resources and capabilities to understand and analyze characteristics of the coronavirus in term of the ways it spreads and virus incubation time. For that, the existing medical features like CT and X-ray images are used. For example, CT-scan images can be used for the detection of lung infection. But the challenges of these features such as the quality of the image and infection characteristics limitate the effectiveness of these features. Using artificial intelligence (AI) tools and computer vision algorithms, the accuracy of detection can be more accurate and can help to overcome these issues. This paper proposes a multi-task deep-learning-based method for lung infection segmentation using CT-scan images. Our proposed method starts by segmenting the lung regions that can be infected. Then, segmenting the infections in these regions. Also, to perform a multi-class segmentation the proposed model is trained using the two-stream inputs. The multi-task learning used in this paper allows us to overcome shortage of labeled data. Also, the multi-input stream allows the model to do the learning on many features that can improve the results. To evaluate the proposed method, many features have been used. Also, from the experiments, the proposed method can segment lung infections with a high degree performance even with shortage of data and labeled images. In addition, comparing with the state-of-the-art method our method achieves good performance results.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "During the first two months, the coronavirus COVID-19 affected thousands of people around the world with a big number of deaths. Wuhan was the first epicenter of the coronavirus, then it began to spread to every continent and most of the countries [1] . The severe acute respiratory syndrome or the acronym known by SARS-CoV-2 is an infectious disease that appeared in late 2019. SARS-CoV-2 is known also by COVID-19 due to its similarity to solar corona from the electron microscopic analysis [2] . The novelty of the Coronavirus and the speed of spread, has created a colossal chaos, impulse among the researchers worlwide to exploit all resources and capabilities to understand and analyze characteristics of the coronavirus in term of spread ways and virus incubation time as well as the exploration of new technics and imposing some temporal procedures to stop the spread speed. The spreading of the virus became unstoppable which prompted the governments to mitigate the impacts of the pandemic using many decisions like stopping the fight and closing the borders and social distancing. However, these policies were not efficient in controlling the spread. To contribute to the implementation of these policies scientists and technology experts attempted to find solutions to stop the speed of the spread. From the technologies used, we can find robots which are used to mitigate the contact between the coronavirus patients and the hospital employees [3] . Also, drones used for monitoring the social distancing and disinfect the public spaces. For the scientists, especially health scientist, searching for medicines and cures that can be helpful to save lives, is the most important mission for them. Whereas the researches in the other domains like computer sciences involved themselves in discovering technics that detect infected persons using the existing medical features like CTscans and X-ray images. Using artificial intelligence (AI) tools and computer vision algorithms, the accuracy of detection can be more accurate and with high precision [4-7, 49, 50] . For that, AI technics can be a good assistant to mitigate the spread by the early detection of the disease. Also, it can be another choice besides the laboratory analysis and allows us to make a large number of tests.", "cite_spans": [{"start": 248, "end": 251, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 494, "end": 497, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 1458, "end": 1461, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 2061, "end": 2074, "text": "[4-7, 49, 50]", "ref_id": null}], "ref_spans": [], "section": "Introduction"}, {"text": "Image processing as well as computer vision combined with AI is a multidisciplinary domain that can be used in different domains including medical, astronomy, agriculture [8] [9] [10] [11] 13] . For the medical field, medical imaging has been used for diagnosing diseases using X-ray and CT images, also for surgery and therapy [3] . Good progress in this field has been reached due to the introduction of many technics like machine learning and deep learning algorithms. This improvement made the computer vision scientists to contribute to finding solutions for rapid diagnostics, prevention, and control. For the same purpose, several approaches have been proposed even when the time is very short.", "cite_spans": [{"start": 171, "end": 174, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 175, "end": 178, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 179, "end": 183, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 184, "end": 188, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 189, "end": 192, "text": "13]", "ref_id": "BIBREF12"}, {"start": 328, "end": 331, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Introduction"}, {"text": "The detection of infection is the first step for the diagnostic of a disease. Using CT images, it can be seen that the appearances of the infected regions are different from the normal regions, so the detection and the extraction of this region automatically can help the doctor for diagnosis in a short time [8] . For the same purpose, a deep learning method for segmentation lung infection for COVID-19 on CT images is proposed. Before starting the learning process, each image has been composed of Structure and texture components. The structure represents the homogeneous part of the images whereas the texture component represented the texture of the image. The structure and texture features are introduced to the encoder-decoder model first for segmenting the region of interest, regions that can be affected. Then, segmenting the specific infected parts on the results of the first segmentation.", "cite_spans": [{"start": 309, "end": 312, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "The remainder of the paper is organized as follows. The literature overview is presented in section 2. The proposed method is presented in section 3. Experiments performed to ", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Method Architecture", "cite_spans": [], "ref_spans": [], "section": "Task"}, {"text": "Ozturk et al. [9] DarkNet, CNN Minaee et al. [10] ResNet18, CNN Apostolopoulos et al. [11] MobileNet, CNN Apostolopoulos et al. [12] ResNet18, VGG19, Inception, Xception Abbas et al. [13] ResNet18, CNN Adhikari et al. [19] DenseNet-based CNN Mobiny et al. [21] CNN Polsinelli et al. [22] SqueezeNet-based CNN Al-karawi et al. [23] Machine learning, FFT-Gabor He et al. [25] CNN Talha et al. [26] CNN Class & segmentation Wu et al. [20] CNN, encoder-decoder Amyar et al. [25] CNN, encoder-decoder Segmentation", "cite_spans": [{"start": 14, "end": 17, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 45, "end": 49, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 86, "end": 90, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 128, "end": 132, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 183, "end": 187, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 218, "end": 222, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 256, "end": 260, "text": "[21]", "ref_id": null}, {"start": 283, "end": 287, "text": "[22]", "ref_id": "BIBREF22"}, {"start": 326, "end": 330, "text": "[23]", "ref_id": "BIBREF23"}, {"start": 369, "end": 373, "text": "[25]", "ref_id": "BIBREF25"}, {"start": 391, "end": 395, "text": "[26]", "ref_id": "BIBREF26"}, {"start": 431, "end": 435, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 470, "end": 474, "text": "[25]", "ref_id": "BIBREF25"}], "ref_spans": [], "section": "Classification"}, {"text": "Fan et al. [27] CNN, partial decoder validate the proposed method are discussed in section 4. The conclusion is provided in section 5.", "cite_spans": [{"start": 11, "end": 15, "text": "[27]", "ref_id": "BIBREF27"}], "ref_spans": [], "section": "Classification"}, {"text": "Recently, medical imaging has gained attention due to its importance to diagnose, monitor, and treat several medical problems. Radiography, a medical imagining technique, uses [14] [15] [16] , CT-scan [17, 18] , and gamma rays to create images of the body that requires internal viewing. analyzing images using different computer vision algorithms provides an alternative for rapid diagnostics and control of many diseases. For COVID-19, image analysis offers a good solution for early detection due to the complexity of laboratory analysis and the importance of early detection that can save lives. For the same purpose, many approaches have been proposed to COVID-19 detection and control using X-ray and CT-scans images of the lung. Also, the use of artificial intelligence makes the precision and the performance of these results convincing. authors in [19] presented COVID-Net architecture that is based on DenseNet to diagnose the COVID-19 infections from X-rays and CT-scans images to decrease the turnaround time of the doctors and check more patients at that point of time. Using the proposed architecture, the infected regions are detected and marked.", "cite_spans": [{"start": 176, "end": 180, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 181, "end": 185, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 186, "end": 190, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 201, "end": 205, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 206, "end": 209, "text": "18]", "ref_id": "BIBREF17"}, {"start": 857, "end": 861, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Related Works"}, {"text": "Working on similar radiographic images, authors in [20] developed a classification and segmentation system for a real-time diagnostic of COVID-19. The presented model combines convolutional neural networks and encoder-decoder networks trained on CT-scans images. The proposed approach succeeds to detect the infected regions with an accuracy of 92%. In the same context, a learning architecture named Detail-Oriented Capsule Networks (DECAPS) has been proposed in [21] . To increases model stability, authors use an Inverted Dynamic Routing mechanism model implementation. The proposed approach detection the infected region without segmenting the region edges. The same technique is used in [22] based on SqueezeNet architecture. The obtained results achieve 89% for performance accuracy wherein [21] the accuracy reaches 87%. To demonstrate the efficiency and the automatic testes of COVID-19 infection, the authors in [23] proposed a machine learning scheme method for CT-scan images analysis for COVID-19 patients. based on the FFT-Gabor scheme, the proposed method for predicting the state of the patient in real-time. The performance accuracy achieves in average 95.37% which represents a convincing rate.", "cite_spans": [{"start": 51, "end": 55, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 464, "end": 468, "text": "[21]", "ref_id": null}, {"start": 692, "end": 696, "text": "[22]", "ref_id": "BIBREF22"}, {"start": 797, "end": 801, "text": "[21]", "ref_id": null}, {"start": 921, "end": 925, "text": "[23]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Related Works"}, {"text": "For developing an accurate method for real-time diagnosing of COVID-19 using CT scan images, deep learning models require a large-scale dataset to train these models, which might be difficult to obtain at this moment. Hence, authors in [24] , build public CT scans dataset people detected positive for COVID-19. Also, a deep-learning-based method has been proposed for classifying the COVID or NON-COVID images. The approach showed an accuracy of 72%, which means that it is not a very accurate method for COVID-19 testing, .", "cite_spans": [{"start": 236, "end": 240, "text": "[24]", "ref_id": "BIBREF24"}], "ref_spans": [], "section": "Related Works"}, {"text": "A multitask deep learning model to identify COVID-19 patient and segmentation of infected regions from chest CT images has been proposed in [25] . The authors used an encoderdecoder model for segmentation and perceptron for classification. The proposed method has the same technic of the method proposed in [20] but using different architectures. The obtained performance accuracy of the proposed method reaches 86%. With an accuracy of 89%, EfficientNet a neural network architecture is proposed for the detection of COVID-19 patients using CT-scan images [26] . For segmenting the infected region in a CT-scan image for COVID-19 patient identification, the authors in [27] proposed a convolutional neural network (CNN) model with a partial decoder. The proposed model performance achieves an accuracy of 73% for detecting the infected regions in a CT-scan image. The authors also train other architectures including Unet [28] ,Unet++ [29] ,Attention-Unet [30] ,Gated-Unet [31] , and Dense-Unet [32] on the same dataset.", "cite_spans": [{"start": 140, "end": 144, "text": "[25]", "ref_id": "BIBREF25"}, {"start": 307, "end": 311, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 557, "end": 561, "text": "[26]", "ref_id": "BIBREF26"}, {"start": 670, "end": 674, "text": "[27]", "ref_id": "BIBREF27"}, {"start": 923, "end": 927, "text": "[28]", "ref_id": "BIBREF28"}, {"start": 936, "end": 940, "text": "[29]", "ref_id": "BIBREF29"}, {"start": 957, "end": 961, "text": "[30]", "ref_id": "BIBREF30"}, {"start": 974, "end": 978, "text": "[31]", "ref_id": null}, {"start": 996, "end": 1000, "text": "[32]", "ref_id": "BIBREF33"}], "ref_spans": [], "section": "Related Works"}, {"text": "All the proposed method attempts to develop a timely and effective method for testing the coronavirus patients. The proposed methods can be classified into two general categories: Classification methods [19, [21] [22] [23] and segmentation [27] of infected region methods. Some of the presented approaches work on two tasks like [20, 25] . Table 1 presents the proposed method for each category as well as the architectures used in each one of them.", "cite_spans": [{"start": 203, "end": 207, "text": "[19,", "ref_id": "BIBREF18"}, {"start": 208, "end": 212, "text": "[21]", "ref_id": null}, {"start": 213, "end": 217, "text": "[22]", "ref_id": "BIBREF22"}, {"start": 218, "end": 222, "text": "[23]", "ref_id": "BIBREF23"}, {"start": 240, "end": 244, "text": "[27]", "ref_id": "BIBREF27"}, {"start": 329, "end": 333, "text": "[20,", "ref_id": "BIBREF19"}, {"start": 334, "end": 337, "text": "25]", "ref_id": "BIBREF25"}], "ref_spans": [{"start": 340, "end": 347, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Related Works"}, {"text": "In this section the proposed approach for lung infection segmentation is provided. The method starts by splitting the texture and structure component of the image, before starting the training of the proposed model for regions of interest extraction. The extraction of these regions is the operation of separation or segmentation of the regions that can contain the infection which are the intern region of the lung. After the extraction of regions of interest, which is performed using the proposed encoder-decoder network proposed, the output of the previous operation is used by the segmentation model which is the sameused for detecting and segmenting the infected part of the lung. A description is provided for each step including stecture-texture decomposition, region of interest extraction and lung infection segmentation.", "cite_spans": [], "ref_spans": [], "section": "Proposed method"}, {"text": "Each image can contain information that includes the form of the object (or the homogenous part) in the image as well as a texture that also contains information that can be July 7, 2020 useful in some computer vision tasks. Here, the texture component is used as input of the proposed encoder-decoder neural networks. for that, a preprocessing is performed to extract the texture component. The structure-texture decomposition method proposed in [36] is adopted using the interval gradient, for adaptive gradient smoothing. Given an image f, it is a technique that splits the image into S+T (f = S+T) of a bounded variation component (Structure) and a component that contains the oscillating part (Texture/Noise) of the image [37, 38] . We applied this technique on the CT-scan images. Then, we use the texture and structure components as an input of the proposed encoder-decoder model. The extraction of the structure component uses gradient rescaling with interval gradients followed by a color handling operation.", "cite_spans": [{"start": 447, "end": 451, "text": "[36]", "ref_id": "BIBREF37"}, {"start": 727, "end": 731, "text": "[37,", "ref_id": "BIBREF38"}, {"start": 732, "end": 735, "text": "38]", "ref_id": "BIBREF39"}], "ref_spans": [], "section": "Structurre and Texture component extraction"}, {"text": "In order to produce a texture-free signal from an input signal I, the gradients within texture regions should be suppressed. Furthermore, the signal should be either increasing or decreasing for all local windows \u2126 r . With these objectives, we use the following equation to rescale the gradients of the input signal with the corresponding interval gradients:", "cite_spans": [], "ref_spans": [], "section": "Structurre and Texture component extraction"}, {"text": "Where(\u2207 I) p represents the rescaled gradient, and w p is the rescaling weight:", "cite_spans": [], "ref_spans": [], "section": "Structurre and Texture component extraction"}, {"text": "Where \u03b5 s is a small constant to prevent numerical instability. Too small values of \u03b5 s would make the algorithm sensitive to noise, introducing unwanted artifacts to filtering results. The sensitivity to noise can be reduced by increasing s but textures may not be completely filtered if \u03b5 s is too big. We set \u03b5 s = 10 \u22124 in our implementation.", "cite_spans": [], "ref_spans": [], "section": "Structurre and Texture component extraction"}, {"text": "For filtering color images, we use the gradient sums of color channels in the gradient rescaling step (Equations (1) and (2)), that is: Figure 3 shows filtering examples to demonstrate the results of structure and texture extraction extraction using the same parameters \u03b5 \u2208 [0.01 2 , 0.03 2 ].", "cite_spans": [], "ref_spans": [{"start": 136, "end": 144, "text": "Figure 3", "ref_id": "FIGREF2"}], "section": "Structurre and Texture component extraction"}, {"text": "Dense pixel-wise classification is a required operation for semantic labeling of images. To achieve an effective semantic segmentation, many architectures have been proposed including FCN [39] , SegNet [40] architectures. In this paper, we proposed a model that is based on the SegNet model which is an encoder-decoder architecture that provides an image output of the same size as the image input.", "cite_spans": [{"start": 188, "end": 192, "text": "[39]", "ref_id": "BIBREF40"}, {"start": 202, "end": 206, "text": "[40]", "ref_id": "BIBREF41"}], "ref_spans": [], "section": "Encoder-decoder architecture"}, {"text": "The encoder part of SegNet is based on the VGG-16 [41] convolutional layers that is composed of 5 blocks, where each one contains 2 to 3 convolutional layers with 3 \u00d7 3 kernels, 1 padding, ReLU, and a batch normalization (BN) [42] . The convolution block is followed by a max-pooling layer of size 2 \u00d7 2. At the end of the encoder, each feature map has H/32, W/32, where the original image resolution is H \u00d7 W .", "cite_spans": [{"start": 50, "end": 54, "text": "[41]", "ref_id": "BIBREF42"}, {"start": 226, "end": 230, "text": "[42]", "ref_id": "BIBREF43"}], "ref_spans": [], "section": "Encoder-decoder architecture"}, {"text": "The decoder part proceeds the operation of upsampling and classification. The main role of this part is the learning of the method of spatial resolution restoration by transforming the encoder features maps into the final labels. The decoder structure is symmetrical with encoder, while the pooling layers in the encoder part are replaced by unpooling layers in the decoder part. The role of convolutional blocks after unpooling layer is to densify the sparse feature maps. This procedure is looped until the feature map reaches the resolution of the input image.", "cite_spans": [], "ref_spans": [], "section": "Encoder-decoder architecture"}, {"text": "The final layer in SegNet as well as in most of the proposed architecture in the same context, SoftMax is used to compute the multinomial loss: ", "cite_spans": [], "ref_spans": [], "section": "Encoder-decoder architecture"}, {"text": "where N is the number of pixels in the input image, k the number of classes and, for a specified pixel i; y i denote its label and PPP the prediction vector. This means that we only minimize the average pixel-wise classification loss without any spatial regularization, as it will be learnt by the network during training.", "cite_spans": [], "ref_spans": [], "section": "Encoder-decoder architecture"}, {"text": "In this paper, we followed the same scenarios, but with two streams as input. The encoder part of the proposed model contains two components. The encoder for texture component and the encoder for structure component. the encoder features map is formed by concatenation of these two encoders. In this architecture, we have 5 encoder blocks. Each block of the encoder is composed of a conv + BN + P ReLU + pooling where, the decoder blocks composed of upsampling +conv +BN +P ReLU . The filter size of each convolutional layer in the encoder part is in the range of 32,64, 128, 256, figure 2 represents the proposed architecture.", "cite_spans": [], "ref_spans": [{"start": 581, "end": 589, "text": "figure 2", "ref_id": "FIGREF1"}], "section": "Encoder-decoder architecture"}, {"text": "The current CT-scan dataset of COVID-19 is very limited in terms of the number of labeled images. Also, the manual segmentation of the infected region of the lung is very difficult because that needs a domain expert to do it like a doctor, and also needs time.", "cite_spans": [], "ref_spans": [], "section": "Lung infection segmentation"}, {"text": "To solve the current problem of limited data, we augment this data using rotation and translation of a large number of images from the labeled data. using the presented encoderdecoder architecture, we first segment the region of interest or the region that can contain the infection. The inside part of the lung is segmented before used in the next stage of lung infection segmentation.", "cite_spans": [], "ref_spans": [], "section": "Lung infection segmentation"}, {"text": "For segmenting the lung infected region using black and white colors, where the white color represents the infected region, we use the segmented region which is the output of the first step of segmentation and the original image as input to the model in the second stage as illustrated in figure 1. The multi-task segmentation is an effective solution regarding the limitation of the size of the data discussed above. Also, the segmentation of the infected region can be used for segmenting these regions using multiclass which represents the next task in this paper.", "cite_spans": [], "ref_spans": [], "section": "Lung infection segmentation"}, {"text": "The segmentation using multiclass or many colors to represent the lung infection can be more helpful for the diagnostic of COVID-19 and also more practical. For that, using the previous segmentation of lung infection and the proposed deep learning model, the multiclass segmentation is performed. Two-stream inputs of the deep learning model represented by the lung infection segmentation results and the results of the first segmentation which is the region of interest. This technique allows learning on the specific region and can be more accurate according to the data limitation. Figure 3 illustrates the input and the output of the deep learning model in this stage for multiclass segmentation.", "cite_spans": [], "ref_spans": [{"start": 585, "end": 593, "text": "Figure 3", "ref_id": "FIGREF2"}], "section": "Multi-class lung infection segmentation"}, {"text": "In this section, we demonstrate the relevance of our proposed method by providing the experimental results. the evaluation has been performed on two segmentation categories including simple segmentation of infected region with black and white, and multi-class labeling using colors. For the first category, we compare our results with a set of state-of-the-art methods, including Unet [28] ,Unet++ [29] ,Attention-Unet [30] ,Gated-Unet [31] , Dense-Unet [32] and Semi-Inf-Net [27] . For multi-class labeling, the obtained results are compared with four state-of-the-art methods such as Semi-Inf-Net [27] , multi-class U-Net [33] , FCN8s [34] and DeepLabV3+ [35] . Also, the results are visualized by presenting some segmented examples using the proposed method as well as state-of-the-art methods.", "cite_spans": [{"start": 385, "end": 389, "text": "[28]", "ref_id": "BIBREF28"}, {"start": 398, "end": 402, "text": "[29]", "ref_id": "BIBREF29"}, {"start": 419, "end": 423, "text": "[30]", "ref_id": "BIBREF30"}, {"start": 436, "end": 440, "text": "[31]", "ref_id": null}, {"start": 454, "end": 458, "text": "[32]", "ref_id": "BIBREF33"}, {"start": 476, "end": 480, "text": "[27]", "ref_id": "BIBREF27"}, {"start": 599, "end": 603, "text": "[27]", "ref_id": "BIBREF27"}, {"start": 624, "end": 628, "text": "[33]", "ref_id": "BIBREF34"}, {"start": 637, "end": 641, "text": "[34]", "ref_id": "BIBREF35"}, {"start": 657, "end": 661, "text": "[35]", "ref_id": "BIBREF36"}], "ref_spans": [], "section": "Experimental results"}, {"text": "The only segmentation dataset of CT-scan images available for COVID-19 is 1 . The dataset consists of 100 axial CT images for 20 COVID-19 patients, collected by the Italian Society of Medical and Interventional Radiology. The dataset contains CT images labeled. There are two categories of labels. The first one labels the regions of interest where the infection can be located. The other one is the specific infected regions labeled with two colored red and green colors. This part contains cropped images for training and testing. The training images composed of 50 images where the infections are labeled with one color (one class) and multi-class (2 colors). The test folder contains 48 images labeled with the same labels as the training. in this paper, we train our multitask model on the two categories For infection segmentation, we use the labeled data and we augment the data by rescaling and rotating the images for getting more data.", "cite_spans": [], "ref_spans": [], "section": "Datasets"}, {"text": "To evaluate the segmentation results of the propsoed method, a set of measures has been exploited including Sorensen-Dice similarity, Sensitivity, Specitivity, Precision and Fmeasure used in [45] [46] [47] [48] . For calculation of these three parameters, the four measures are required, namely true positive (TP), false positive (FP), true negative (TN), and false negative (FN). The true positive (TP) represents the number of pixels being correctly identifed. True negative (TN) describe the number of non-lung infection pixels being correctly identifed as non-lung infection. False positive (FP) denotes the number of non-lung infection being wrongly classifed as lung infection, whereas false negative (FN) means the lung infection pixels being wrongly classifed as non-lung infecttion.", "cite_spans": [{"start": 191, "end": 195, "text": "[45]", "ref_id": "BIBREF46"}, {"start": 196, "end": 200, "text": "[46]", "ref_id": "BIBREF47"}, {"start": 201, "end": 205, "text": "[47]", "ref_id": "BIBREF48"}, {"start": 206, "end": 210, "text": "[48]", "ref_id": "BIBREF49"}], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "Sorensen-Dice similarity: Let consider that A is segmented regions that we need to assess the quality, B is ground truth. The Sorensen-Dice similarity [46] is computed as ", "cite_spans": [{"start": 151, "end": 155, "text": "[46]", "ref_id": "BIBREF47"}], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "The value of the Sorensen-Dice similarity metric is between 0 and 1. The higher the Sorensen-Dice value, the better the segmentation result.", "cite_spans": [], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "Sensitivity: Sensitivity, defned as the ratio of correctly identifed lung infection to the total number of lung infection pixels, is computed as", "cite_spans": [], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "Specifcity: Specifcity, defned as the ratio of correctly detected non-lung unfection to the total number of non-lung infection pixels, is measured as", "cite_spans": [], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "Precision: Precision gives the percentage of unnecessary positives through the compared total number of positive pixels in the detected binary objects mask [48] P recision = T P T P + F P", "cite_spans": [{"start": 156, "end": 160, "text": "[48]", "ref_id": "BIBREF49"}], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "F-measure: the weighted harmonic mean of precision and sensitivity, computes the quality of detection.", "cite_spans": [], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "Mean Absolute Error (MAE) is also used to evaluate the performance of the proposed model:", "cite_spans": [], "ref_spans": [], "section": "Evaluation metrics"}, {"text": "The first step of our proposed model is to segment the regions that can contain the infection from lung images. This step is sued for segmenting the lung infection regions in the second part of the proposed method. This step is an accurate preprocessing operation for segmenting the lung infection region. Figure 4 illustrates some obtained results for the region of interest segmentation by presenting the original image with the ground truth as well as the obtained results. We can observe that the proposed model gives promising segmentation results. Also, comparing with the ground truth, we obtain a segmentation without fold segmented regions.", "cite_spans": [], "ref_spans": [{"start": 306, "end": 314, "text": "Figure 4", "ref_id": "FIGREF3"}], "section": "Discussion"}, {"text": "To evaluate the performance of the proposed method for lung infection segmentation Figure 5 show some examples of the obtained results. From the visualized results we can observe that the proposed model can detect the infection effectively with some errors that can be considered negligible. Also, the segmentation results are close to the ground truth show in the second line of figure 5 . The success of the proposed method to label the infection is owed to the used architecture that uses two-stream input which allow robust learning. Also, the use of the results of the region of interest as input beside the original image gives the model a specification of the region that can contain the infection. The robustness of the presented approach is shown also in the presented metrics that demonstrate the performance evaluation including Dice, Sensitivity, specificity, precision, and F-measure. For example, for the first CT-scan image, our approach can segment with a high precision value which achieves 84% for the F-measure metric which is a convincing value. The presented results can be improved using a preprocessing on the results images like the morphology operations.", "cite_spans": [], "ref_spans": [{"start": 83, "end": 91, "text": "Figure 5", "ref_id": "FIGREF4"}, {"start": 380, "end": 388, "text": "figure 5", "ref_id": "FIGREF4"}], "section": "Discussion"}, {"text": "In order to assert the results represented by the binary image in Fig. 5 , the qualitative and quantitative results obtained by each method are represented in Table 2 . From these results, we can see that the proposed method gives high performance when compared with the state-of-art methods. The effectiveness of the proposed method comes from the use of multi-task learning and the use of two-stream input for our model.", "cite_spans": [], "ref_spans": [{"start": 66, "end": 72, "text": "Fig. 5", "ref_id": "FIGREF4"}, {"start": 159, "end": 166, "text": "Table 2", "ref_id": "TABREF2"}], "section": "Discussion"}, {"text": "The multi-class infection labeling results has also been illustrated in Figure 6 . As shown in the figure the proposed method performs an accurate segmentation of the lung infection using multi-class labeling. The best result comes from the succession of tasks for performing the multi-class segmentation. The use of the results of unit-class segmentation with the original image leads to a precise segmentation of the lung infection. The evaluation using different metric in Table 3 also demonstrate the advantage of the proposed method compared with the other existing methods. For example, the semi-inf-Net method succeed to obtain close results due to the multi-task learning model. In contrast to the other model that are used as it is like UNet of FC8s models.", "cite_spans": [], "ref_spans": [{"start": 72, "end": 80, "text": "Figure 6", "ref_id": "FIGREF5"}, {"start": 476, "end": 483, "text": "Table 3", "ref_id": "TABREF3"}], "section": "Discussion"}, {"text": "In this paper, a lung infection segmentation method for COVID-19 has been proposed. Based on encoder-decoder networks on CT-scan images exploit the computer vision techniques to identify the lung infected regions for COVID-19 patients. Due to the shortage of data at this moment, multi-task learning has been performed including the use of two-stream as inputs of the deep learning model. Also, using computer vision features like structure and texture component of the images that help for good extraction of the region of interest that can contain infections has also been utilized. Different segmentation has been performed including the binary segmentation and multi-class segmentation of lung infected regions. comparing the proposed approach with the state-of-the-art method, the experiment shows an accurate segmentation of the lung infection region in both binary and multiclass segmentation. The obtained results can be improved by using more data for training and more labeled data for multi-class segmentation, which represents our future works.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "This publication was made by rEdicting RIsk earLy in COVID-19 project (QUERG-CENG-2020-1). The statements made herein are solely the responsibility of the authors.", "cite_spans": [], "ref_spans": [], "section": "Acknowledgements"}, {"text": "The authors declare that they have no competing interests.", "cite_spans": [], "ref_spans": [], "section": "Conflict interests"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions", "authors": [{"first": "Thanh", "middle": [], "last": "Nguyen", "suffix": ""}, {"first": "", "middle": [], "last": "Thi", "suffix": ""}], "year": null, "venue": "", "volume": "10", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Computer Vision for COVID-19 Control: A Survey", "authors": [{"first": "Anwaar", "middle": [], "last": "Ulhaq", "suffix": ""}, {"first": "", "middle": [], "last": "Khan", "suffix": ""}, {"first": "", "middle": [], "last": "Asim", "suffix": ""}, {"first": "", "middle": [], "last": "Gomes", "suffix": ""}, {"first": "", "middle": [], "last": "Douglas", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.09420"]}}, "BIBREF2": {"ref_id": "b2", "title": "Leveraging Data Science To Combat COVID-19: A Comprehensive Review", "authors": [{"first": "Siddique", "middle": [], "last": "Latif", "suffix": ""}, {"first": "", "middle": [], "last": "Usman", "suffix": ""}, {"first": "", "middle": [], "last": "Muhammad", "suffix": ""}, {"first": "", "middle": [], "last": "Manzoor", "suffix": ""}, {"first": "", "middle": [], "last": "Sanaullah", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Visual object tracking via the local soft cosine similarity", "authors": [{"first": "Driss", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "Omar", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "Hamid", "middle": [], "last": "Tairi", "suffix": ""}], "year": 2018, "venue": "Pattern Recognition Letters", "volume": "110", "issn": "", "pages": "79--85", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "LFR face dataset:Left-Front-Right dataset for poseinvariant face recognition in the wild", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "N", "middle": [], "last": "Almaadeed", "suffix": ""}, {"first": "S", "middle": [], "last": "Al-Maadeed", "suffix": ""}], "year": 2020, "venue": "2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)", "volume": "", "issn": "", "pages": "124--130", "other_ids": {"DOI": ["10.1109/ICIoT48696.2020.9089530"]}}, "BIBREF5": {"ref_id": "b5", "title": "An image steganography approach based on kleast significant bits (k-LSB)", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "N", "middle": [], "last": "Almaadeed", "suffix": ""}, {"first": "S", "middle": [], "last": "Al-Maadeed", "suffix": ""}], "year": 2020, "venue": "2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)", "volume": "", "issn": "", "pages": "131--135", "other_ids": {"DOI": ["10.1109/ICIoT48696.2020.9089566"]}}, "BIBREF6": {"ref_id": "b6", "title": "Mhad:multi-human action dataset", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "N", "middle": [], "last": "Almaadeed", "suffix": ""}, {"first": "S", "middle": [], "last": "Al-Maadeed", "suffix": ""}], "year": 2020, "venue": "Advances in Intelligent Systemsand Computing", "volume": "1041", "issn": "", "pages": "333--341", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "COVID-19: A Survey on Public Medical Imaging Data Resources", "authors": [{"first": "Roman", "middle": [], "last": "Kalkreuth", "suffix": ""}, {"first": "Paul", "middle": [], "last": "Kaufmann", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.04569"]}}, "BIBREF8": {"ref_id": "b8", "title": "Automated detection of COVID-19 cases using deep neural networks with X-ray images", "authors": [{"first": "Tulin", "middle": [], "last": "Ozturk", "suffix": ""}, {"first": "", "middle": [], "last": "Talo", "suffix": ""}, {"first": "Yildirim", "middle": [], "last": "Muhammed", "suffix": ""}, {"first": "Eylul", "middle": [], "last": "Azra", "suffix": ""}], "year": 2020, "venue": "Computers in Biology and Medicine", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Deep-covid: Predicting covid-19 from chest x-ray images using deep transfer learning", "authors": [{"first": "", "middle": [], "last": "Minaee", "suffix": ""}, {"first": "", "middle": [], "last": "Shervin", "suffix": ""}, {"first": "", "middle": [], "last": "Kafieh", "suffix": ""}, {"first": "", "middle": [], "last": "Rahele", "suffix": ""}, {"first": "", "middle": [], "last": "Sonka", "suffix": ""}, {"first": "", "middle": [], "last": "Milan", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.09363"]}}, "BIBREF10": {"ref_id": "b10", "title": "Extracting possibly representative COVID-19 Biomarkers from X-Ray images with Deep Learning approach and image data related to Pulmonary Diseases", "authors": [{"first": "Ioannis", "middle": ["D"], "last": "Apostolopoulos", "suffix": ""}, {"first": "", "middle": [], "last": "Aznaouridis", "suffix": ""}, {"first": "I", "middle": [], "last": "Sokratis", "suffix": ""}, {"first": "Tzani", "middle": [], "last": "Et", "suffix": ""}, {"first": "A", "middle": [], "last": "Mpesiana", "suffix": ""}], "year": 2020, "venue": "Journal of Medical and Biological Engineering", "volume": "", "issn": "", "pages": "1--8", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Covid-19: automatic detection from x-ray images utilizing transfer learning with convolutional neural networks", "authors": [{"first": "Ioannis", "middle": ["D"], "last": "Apostolopoulos", "suffix": ""}, {"first": "", "middle": [], "last": "Mpesiana", "suffix": ""}, {"first": "A", "middle": [], "last": "Tzani", "suffix": ""}], "year": 2020, "venue": "Physical and Engineering Sciences in Medicine", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network", "authors": [{"first": "Asmaa", "middle": [], "last": "Abbas", "suffix": ""}, {"first": "Mohammed", "middle": ["M"], "last": "Abdelsamea", "suffix": ""}, {"first": "Gaber", "middle": [], "last": "Et", "suffix": ""}, {"first": "Mohamed", "middle": [], "last": "Medhat", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.13815"]}}, "BIBREF13": {"ref_id": "b13", "title": "COVID-19 image data collection", "authors": [{"first": "Joseph", "middle": [], "last": "Cohen", "suffix": ""}, {"first": "", "middle": [], "last": "Paul", "suffix": ""}, {"first": "Paul", "middle": [], "last": "Morrison", "suffix": ""}, {"first": "Lan", "middle": [], "last": "Dao", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.11597"]}}, "BIBREF14": {"ref_id": "b14", "title": "COVID-19 Patients Lungs X Ray Images 10000", "authors": [], "year": null, "venue": "", "volume": "", "issn": "", "pages": "2020--2024", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Can AI help in screening Viral and COVID-19 pneumonia", "authors": [{"first": "M", "middle": ["E H"], "last": "Chowdhury", "suffix": ""}, {"first": "T", "middle": [], "last": "Rahman", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "COVID-CT-Dataset: a CT scan dataset about COVID-19", "authors": [{"first": "J", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "X", "middle": [], "last": "He", "suffix": ""}, {"first": "P", "middle": [], "last": "Xie", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "COVID-19 CT segmentation dataset", "authors": [], "year": null, "venue": "", "volume": "", "issn": "", "pages": "2020--2024", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Infection Severity Detection of CoVID19 from X-Rays and CT Scans Using Artificial Intelligence", "authors": [{"first": "Nimai Chand", "middle": [], "last": "Adhikari", "suffix": ""}, {"first": "", "middle": [], "last": "Das", "suffix": ""}], "year": 2020, "venue": "International Journal of Computer (IJC)", "volume": "38", "issn": "1", "pages": "73--92", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "An explainable COVID-19 diagnosis system by joint classification and segmentation", "authors": [{"first": "Yu-Huan", "middle": [], "last": "Wu", "suffix": ""}, {"first": "", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Mei", "middle": [], "last": "Shang-Hua", "suffix": ""}, {"first": "", "middle": [], "last": "Jie", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.07054"]}}, "BIBREF21": {"ref_id": "b21", "title": "Detection Using CT Scans with Detail-Oriented Capsule Networks", "authors": [], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.07407"]}}, "BIBREF22": {"ref_id": "b22", "title": "A Light CNN for detecting COVID-19 from CT scans of the chest", "authors": [{"first": "Matteo", "middle": [], "last": "Polsinelli", "suffix": ""}, {"first": "Luigi", "middle": [], "last": "Cinque", "suffix": ""}, {"first": "Giuseppe", "middle": [], "last": "Placidi", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.12837"]}}, "BIBREF23": {"ref_id": "b23", "title": "Machine Learning Analysis of Chest CT Scan Images as a Complementary Digital Test of Coronavirus (COVID-19) Patients. medRxiv", "authors": [{"first": "", "middle": [], "last": "Al-Karawi", "suffix": ""}, {"first": "Al-Zaidi", "middle": [], "last": "Dhurgham", "suffix": ""}, {"first": "", "middle": [], "last": "Shakir", "suffix": ""}, {"first": "", "middle": [], "last": "Polus", "suffix": ""}, {"first": "", "middle": [], "last": "Nisreen", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Sample-Efficient Deep Learning for COVID-19 Diagnosis Based on CT Scans. medRxiv", "authors": [{"first": "", "middle": [], "last": "He", "suffix": ""}, {"first": "Yang", "middle": [], "last": "Xuehai", "suffix": ""}, {"first": "", "middle": [], "last": "Xingyi", "suffix": ""}, {"first": "", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "", "middle": [], "last": "Shanghang", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "Multi-task Deep Learning Based CT Imaging Analysis For COVID-19: Classification and Segmentation. medRxiv", "authors": [{"first": "Amine", "middle": [], "last": "Amyar", "suffix": ""}, {"first": "Romain", "middle": [], "last": "Modzelewski", "suffix": ""}, {"first": "Su", "middle": [], "last": "Ruan", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "Deep learning based diagnosis of COVID-19 using chest CT-scan images", "authors": [{"first": "Talha", "middle": [], "last": "Anwar", "suffix": ""}, {"first": "Seemab", "middle": [], "last": "Zakir", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Images", "authors": [{"first": "", "middle": [], "last": "Fan", "suffix": ""}, {"first": "", "middle": [], "last": "Deng-Ping", "suffix": ""}, {"first": "", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "J", "middle": ["I"], "last": "Tao", "suffix": ""}, {"first": "", "middle": [], "last": "Ge-Peng", "suffix": ""}], "year": 2020, "venue": "IEEE Transactions on Medical Imaging", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "U-Net: Convolutional networks for biomedical image segmentation", "authors": [{"first": "O", "middle": [], "last": "Ronneberger", "suffix": ""}, {"first": "P", "middle": [], "last": "Fischer", "suffix": ""}, {"first": "T", "middle": [], "last": "Brox", "suffix": ""}], "year": 2015, "venue": "MICCAI", "volume": "", "issn": "", "pages": "234--241", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "UNet++: A nested U-Net architecture for medical image segmentation", "authors": [{"first": "Z", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "M", "middle": ["M R"], "last": "Siddiquee", "suffix": ""}, {"first": "N", "middle": [], "last": "Tajbakhsh", "suffix": ""}, {"first": "J", "middle": [], "last": "Liang", "suffix": ""}], "year": 2019, "venue": "IEEE Transactions on Medical Imaging", "volume": "", "issn": "", "pages": "3--11", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "Attention U-Net: Learning Where to Look for the Pancreas", "authors": [{"first": "O", "middle": [], "last": "Oktay", "suffix": ""}, {"first": "J", "middle": [], "last": "Schlemper", "suffix": ""}], "year": 2018, "venue": "International Conference on Medical Imaging with Deep Learning", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF32": {"ref_id": "b32", "title": "Learning to leverage salient regions in medical images", "authors": [], "year": 2019, "venue": "Medical Image Analysis", "volume": "53", "issn": "", "pages": "197--207", "other_ids": {}}, "BIBREF33": {"ref_id": "b33", "title": "H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation From CT Volumes", "authors": [{"first": "X", "middle": [], "last": "Li", "suffix": ""}, {"first": "H", "middle": [], "last": "Chen", "suffix": ""}, {"first": "X", "middle": [], "last": "Qi", "suffix": ""}, {"first": "Q", "middle": [], "last": "Dou", "suffix": ""}, {"first": "C", "middle": [], "last": "Fu", "suffix": ""}, {"first": "P", "middle": [], "last": "Heng", "suffix": ""}], "year": 2018, "venue": "IEEE Transactions on Medical Imaging", "volume": "37", "issn": "12", "pages": "2663--2674", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "U-Net: Convolutional networks for biomedical image segmentation", "authors": [{"first": "O", "middle": [], "last": "Ronneberger", "suffix": ""}, {"first": "P", "middle": [], "last": "Fischer", "suffix": ""}, {"first": "T", "middle": [], "last": "Brox", "suffix": ""}], "year": 2015, "venue": "MICCAI", "volume": "", "issn": "", "pages": "234--241", "other_ids": {}}, "BIBREF35": {"ref_id": "b35", "title": "Fully convolutional networks for semantic segmentation", "authors": [{"first": "J", "middle": [], "last": "Long", "suffix": ""}, {"first": "E", "middle": [], "last": "Shelhamer", "suffix": ""}, {"first": "T", "middle": [], "last": "Darrell", "suffix": ""}], "year": 2015, "venue": "CVPR", "volume": "", "issn": "", "pages": "3431--3440", "other_ids": {}}, "BIBREF36": {"ref_id": "b36", "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation", "authors": [{"first": "L.-C", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "G", "middle": [], "last": "Papandreou", "suffix": ""}, {"first": "F", "middle": [], "last": "Schroff", "suffix": ""}, {"first": "H", "middle": [], "last": "Adam", "suffix": ""}], "year": 2018, "venue": "in ECCV", "volume": "", "issn": "", "pages": "801--818", "other_ids": {}}, "BIBREF37": {"ref_id": "b37", "title": "Color and scale: the spatial structure of color images", "authors": [{"first": "Jan-Mark", "middle": [], "last": "Geusebroek", "suffix": ""}, {"first": "R", "middle": [], "last": "Van Den Boomgaard", "suffix": ""}, {"first": "A", "middle": ["W M"], "last": "Smeulders", "suffix": ""}, {"first": "A", "middle": [], "last": "Dev", "suffix": ""}], "year": 2000, "venue": "Proceeding of the 6th European Conference on Computer Vision", "volume": "1", "issn": "", "pages": "331--341", "other_ids": {}}, "BIBREF38": {"ref_id": "b38", "title": "Motion detection based on the combining of the background subtraction and the structure-texture decomposition", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "D", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "H", "middle": [], "last": "Tairi", "suffix": ""}], "year": 2015, "venue": "Optik-International Journal for Light and Electron Optics", "volume": "126", "issn": "", "pages": "5992--5997", "other_ids": {}}, "BIBREF39": {"ref_id": "b39", "title": "Moving object detection zone using a block-based background model", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "A", "middle": [], "last": "Abbad", "suffix": ""}, {"first": "D", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "H", "middle": [], "last": "Tairi", "suffix": ""}], "year": 2017, "venue": "IET Comput. Vis", "volume": "12", "issn": "1", "pages": "86--94", "other_ids": {}}, "BIBREF40": {"ref_id": "b40", "title": "Fully convolutional networks for semantic segmentation", "authors": [{"first": "J", "middle": [], "last": "Long", "suffix": ""}, {"first": "E", "middle": [], "last": "Shelhamer", "suffix": ""}, {"first": "T", "middle": [], "last": "Darrell", "suffix": ""}], "year": 2015, "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "volume": "", "issn": "", "pages": "3431--3440", "other_ids": {}}, "BIBREF41": {"ref_id": "b41", "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "authors": [{"first": "V", "middle": [], "last": "Badrinarayanan", "suffix": ""}, {"first": "A", "middle": [], "last": "Kendall", "suffix": ""}, {"first": "R", "middle": [], "last": "Cipolla", "suffix": ""}], "year": 2017, "venue": "IEEE transactions on pattern analysis and machine intelligence", "volume": "39", "issn": "", "pages": "2481--2495", "other_ids": {}}, "BIBREF42": {"ref_id": "b42", "title": "Very deep convolutional networks for large-scale image recognition", "authors": [{"first": "Karen", "middle": [], "last": "Simonyan", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Zisserman", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1409.1556"]}}, "BIBREF43": {"ref_id": "b43", "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "authors": [{"first": "S", "middle": [], "last": "Ioffe", "suffix": ""}, {"first": "C", "middle": [], "last": "Szegedy", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 32nd International Conference on Machine Learning", "volume": "", "issn": "", "pages": "448--456", "other_ids": {}}, "BIBREF44": {"ref_id": "b44", "title": "Large-scale screening of covid-19 from community acquired pneumonia using infection size-aware classification", "authors": [{"first": "Feng", "middle": [], "last": "Shi", "suffix": ""}, {"first": "", "middle": [], "last": "Xia", "suffix": ""}, {"first": "", "middle": [], "last": "Liming", "suffix": ""}, {"first": "", "middle": [], "last": "Shan", "suffix": ""}, {"first": "", "middle": [], "last": "Fei", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.09860"]}}, "BIBREF45": {"ref_id": "b45", "title": "Lung infection quantification of covid-19 in ct images with deep learning", "authors": [{"first": "Fei", "middle": [], "last": "Shan", "suffix": ""}, {"first": "Yaozong", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Jun", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Weiya", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Nannan", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Miaofei", "middle": [], "last": "Han", "suffix": ""}, {"first": "Zhong", "middle": [], "last": "Xue", "suffix": ""}, {"first": "Yuxin", "middle": [], "last": "Shi", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.04655"]}}, "BIBREF46": {"ref_id": "b46", "title": "Moving object detection using a background modeling based on entropy theory and quad-tree decomposition", "authors": [{"first": "Omar", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "Elkah", "middle": [], "last": "Driss", "suffix": ""}, {"first": "", "middle": [], "last": "Samah", "suffix": ""}], "year": 2016, "venue": "Journal of Electronic Imaging", "volume": "25", "issn": "", "pages": "", "other_ids": {}}, "BIBREF47": {"ref_id": "b47", "title": "BLOOD VESSELS SEGMENTATION METHOD FOR RETINAL FUN-DUS IMAGES BASED ON ADAPTIVE PRINCIPAL CURVATURE AND IMAGE DERIVATIVE OPERATORS", "authors": [{"first": "D", "middle": ["N H"], "last": "Thanh", "suffix": ""}], "year": 2019, "venue": "ISPRS -International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "volume": "", "issn": "", "pages": "211--229", "other_ids": {"DOI": ["10.5194/isprs-archives-xlii-2-w12-211-2019"]}}, "BIBREF48": {"ref_id": "b48", "title": "Boosting sensitivity of a retinal vessel segmentation algorithm", "authors": [{"first": "M", "middle": ["A"], "last": "Khan", "suffix": ""}, {"first": "T", "middle": ["M"], "last": "Khan", "suffix": ""}, {"first": "T", "middle": ["A"], "last": "Soomro", "suffix": ""}, {"first": "N", "middle": [], "last": "Mir", "suffix": ""}, {"first": "J", "middle": [], "last": "Gao", "suffix": ""}], "year": 2019, "venue": "Pattern Analysis and Applications", "volume": "22", "issn": "2", "pages": "583--599", "other_ids": {}}, "BIBREF49": {"ref_id": "b49", "title": "ELCVIA: Electronic Letters on Computer Vision and ImageAnalysis", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "A", "middle": [], "last": "Abbad", "suffix": ""}, {"first": "D", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "J", "middle": [], "last": "Riffi", "suffix": ""}, {"first": "H", "middle": [], "last": "Tairi", "suffix": ""}], "year": 2016, "venue": "", "volume": "15", "issn": "", "pages": "17--31", "other_ids": {"DOI": ["10.5565/rev/elcvia.855"]}}, "BIBREF50": {"ref_id": "b50", "title": "Moving object detection using a background modeling based on entropy theory and quad-tree decomposition", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "D", "middle": [], "last": "Moujahid", "suffix": ""}, {"first": "S", "middle": [], "last": "Elkah", "suffix": ""}, {"first": "H", "middle": [], "last": "Tairi", "suffix": ""}], "year": 2016, "venue": "J. Electron. Imaging", "volume": "25", "issn": "", "pages": "", "other_ids": {}}, "BIBREF51": {"ref_id": "b51", "title": "Image inpainting: A review", "authors": [{"first": "O", "middle": [], "last": "Elharrouss", "suffix": ""}, {"first": "N", "middle": [], "last": "Almaadeed", "suffix": ""}, {"first": "S", "middle": [], "last": "Al-Maadeed", "suffix": ""}, {"first": "Y", "middle": [], "last": "Akbari", "suffix": ""}], "year": 2019, "venue": "Neural Processing Letters", "volume": "", "issn": "", "pages": "1--22", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Flowchart of the proposed method.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Structure and Texture extraction results: Left: original image. Midle: Structure component. Right: Texture component", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Multi-class segemrntation of the lung infected regions", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Region of interest segmentation results. First row : original images. Second row: groundtruth. third row: segmentation using the propsoed method.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "infection region segmentation . First row : original images. Second row: groundtruth. Third row: segmentation using the propsoed method. fourth row: evaluation results", "latex": null, "type": "figure"}, "FIGREF5": {"text": "Segmentation results using the proposed method. First row: original images. Second row: binary ground truth. Third row: multi-class ground truth. Fifth row: Our binary segmentation results. Sixth row: Our multi-class segmentation results. of images. We train the first part of our model on the 2000 images for region segmentation.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Categories of COVID-19 methods", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Task </td><td>Method </td><td>Architecture\n</td></tr><tr><td>Ozturk et al. [9] </td><td>DarkNet, CNN\n</td></tr><tr><td>\u00a0</td><td>Minaee et al. [10] </td><td>ResNet18, CNN\n</td></tr><tr><td>\u00a0</td><td>Apostolopoulos et al. [11] </td><td>MobileNet, CNN\n</td></tr><tr><td>\u00a0</td><td>Apostolopoulos et al. [12] </td><td>ResNet18, VGG19, Inception, Xception\n</td></tr><tr><td>Classification\n</td><td>Abbas et al. [13] </td><td>ResNet18, CNN\n</td></tr><tr><td>Adhikari et al. [19] </td><td>DenseNet-based CNN\n</td></tr><tr><td>Mobiny et al. [21] </td><td>CNN\n</td></tr><tr><td>\u00a0</td><td>Polsinelli et al. [22] </td><td>SqueezeNet-based CNN\n</td></tr><tr><td>\u00a0</td><td>Al-karawi et al.[23] </td><td>Machine learning, FFT-Gabor\n</td></tr><tr><td>\u00a0</td><td>He et al. [25] </td><td>CNN\n</td></tr><tr><td>Class &amp;\nsegmentation\n</td><td>Talha et al.[26] </td><td>CNN\n</td></tr><tr><td>Wu et al. [20] </td><td>CNN, encoder-decoder\n</td></tr><tr><td>Amyar et al. [25] </td><td>CNN, encoder-decoder\n</td></tr><tr><td>Segmentation </td><td>Fan et al. [27] </td><td>CNN, partial decoder\n</td></tr></table></body></html>"}, "TABREF2": {"text": "Quantitative results of binary infection regions segmentation on COVID-SemiSeg DATASET", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Method </td><td>Dice </td><td>Sensitivity </td><td>Specitivity </td><td>Precision </td><td>Fmeasure </td><td>MAE\n</td></tr><tr><td>U-Net [28] </td><td>0.439 </td><td>0.534 </td><td>0.858 </td><td>- </td><td>- </td><td>0.186\n</td></tr><tr><td>Attention-UNet [30] </td><td>0.583 </td><td>0.637 </td><td>0.921 </td><td>- </td><td>- </td><td>0.112\n</td></tr><tr><td>Gated-UNet [31] </td><td>0.623 </td><td>0.658 </td><td>0.926 </td><td>- </td><td>- </td><td>0.102\n</td></tr><tr><td>Dense-UNet [32] </td><td>0.515 </td><td>0.594 </td><td>0.840 </td><td>- </td><td>- </td><td>0.184\n</td></tr><tr><td>U-Net++ [29] </td><td>0.422 </td><td>0.379 </td><td>0.976 </td><td>- </td><td>- </td><td>0.120\n</td></tr><tr><td>Semi-Inf-Net [27] </td><td>0.739 </td><td>0.725 </td><td>0.960 </td><td>- </td><td>- </td><td>0.064\n</td></tr><tr><td>Propsed method </td><td>0.786 </td><td>0.711 </td><td>0.993 </td><td>0.856 </td><td>0.784 </td><td>0.076\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Quantitative results of infection regions using multi-class segmentation on COVID-SemiSeg DATASET", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Method </td><td>Dice </td><td>Sensitivity </td><td>Specitivity </td><td>Precision </td><td>Fmeasure </td><td>MAE\n</td></tr><tr><td>multi-class U-Net [33] </td><td>0.422 </td><td>.0379 </td><td>0.976 </td><td>- </td><td>- </td><td>0.066\n</td></tr><tr><td>DeepLabV3+ [35] </td><td>0.341 </td><td>0.512 </td><td>0.766 </td><td>- </td><td>- </td><td>0.117\n</td></tr><tr><td>FC8s [34] </td><td>0.375 </td><td>0.403 </td><td>0.811 </td><td>- </td><td>- </td><td>0.076\n</td></tr><tr><td>Semi-Inf-Net [27] </td><td>0.541 </td><td>0.564 </td><td>0.967 </td><td>- </td><td>- </td><td>0.057\n</td></tr><tr><td>Propsed method </td><td>0.640 </td><td>0.630 </td><td>0.953 </td><td>0.561 </td><td>0.640 </td><td>0.062\n</td></tr></table></body></html>"}}, "back_matter": []}